ssh skundu@129.150.69.91

#== works
# pyspark  --num-executors=16 --executor-cores=8 --executor-memory=23G
#== Works as well
# pyspark  --num-executors=10 --executor-cores=8 --executor-memory=23G


from pyspark.sql.types import *
from pyspark.sql.functions import *
from pyspark.sql.session import SparkSession
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
from pyspark.ml import Pipeline
from pyspark.storagelevel import StorageLevel

from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit
from pyspark.ml.feature import StandardScaler

from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import OneHotEncoder
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator
from pyspark.ml.regression import LinearRegression

from pyspark.sql import functions as F
import pyspark.sql.functions as F
import sys

#csv = spark.read.csv('/user/skundu/iowa_Liquor_Sales.csv', inferSchema=True, header=True)
 
csv = spark.read.csv('/user/skundu/sample.csv', inferSchema=True, header=True)


csv.show(3)

csv.registerTempTable("temp")


csv1 = sqlContext.sql("select Pack, `Bottle Volume (ml)` as BottleVolumeInMl, `State Bottle Cost` as StateBottleCost, `State Bottle Retail` as StateBottleRetail, `Bottles Sold` as BottlesSold, `Sale (Dollars)` as SaleInDollars, `Volume Sold (Liters)` as VolumeSoldInLitres from temp")

df1 = csv1.filter(csv1.StateBottleCost.isNotNull())

df2 = df1.filter(df1.StateBottleRetail.isNotNull())

df3 = df2.filter(df2.BottleVolumeInMl.isNotNull())

df4 = df3.filter(df3.Pack.isNotNull())

df5 = df4.filter(df4.BottlesSold.isNotNull())

df6 = df5.filter(df5.SaleInDollars.isNotNull())

df7 = df6.filter(df6.VolumeSoldInLitres.isNotNull())

data = df7.select(col("Pack").cast(DoubleType()), col("BottleVolumeInMl").cast(DoubleType()), "StateBottleCost", "StateBottleRetail", col("BottlesSold").cast(DoubleType()), "VolumeSoldInLitres", col("SaleInDollars").alias("label"))

# JWoo5: persist the dataframes with DISK_ONLY
# data.persist(StorageLevel.MEMORY_AND_DISK)
data.persist(StorageLevel.DISK_ONLY)

data.show(5)

splits = data.randomSplit([0.7, 0.3])
train = splits[0]
test = splits[1].withColumnRenamed("label", "trueLabel")
 
# JWoo5: persist the dataframes
train.persist(StorageLevel.DISK_ONLY)
test.persist(StorageLevel.DISK_ONLY)
 
print ("Training Rows:", train.count(), "Testing Rows:", test.count())

train.show(20)

assembler = VectorAssembler(inputCols = ["Pack", "BottleVolumeInMl","StateBottleCost", "StateBottleRetail", "BottlesSold", "VolumeSoldInLitres"], outputCol="features")

temp_train = assembler.transform(train)

temp_train.select("features").show(5)

temp_train.show(5)

temp_train.persist(StorageLevel.DISK_ONLY)



standardScaler = StandardScaler(inputCol="features", outputCol="scaled")

lr_train = standardScaler.fit(temp_train).transform(temp_train)

lr_train.show(5)

lr_train.persist(StorageLevel.DISK_ONLY)

df_train = lr_train.select("label", "scaled")

df_train.show(5)

df_train.persist(StorageLevel.DISK_ONLY)




temp_test = assembler.transform(test)

temp_test.select("features").show(5)

temp_test.show(5)

temp_test.persist(StorageLevel.DISK_ONLY)

lr_test = standardScaler.fit(temp_test).transform(temp_test)

lr_test.show(5)

lr_test.persist(StorageLevel.DISK_ONLY)

df_test = lr_test.select("trueLabel", "scaled")

df_test.persist(StorageLevel.DISK_ONLY)


lr = LinearRegression(labelCol="label", featuresCol="scaled", maxIter=10, regParam=.01)

#lr = LinearRegression(labelCol="label", featuresCol="features", maxIter=10, regParam=0.3)

pipeline = Pipeline(stages=[lr])

paramGrid_tvs = ParamGridBuilder().addGrid(lr.regParam, [0.3, 0.1, 0.01]).addGrid(lr.maxIter, [10, 5]).build()

tvs = TrainValidationSplit(estimator=pipeline, evaluator=RegressionEvaluator(), estimatorParamMaps=paramGrid_tvs, trainRatio=0.8)

#piplineModel_tvs = tvs.fit(train)
lr_model = tvs.fit(df_train)

#model = pipeline.fit(train)

#prediction = piplineModel_tvs.transform(test)
lr_prediction = lr_model.transform(df_test)

#prediction = model.transform(test)


#predicted_tvs = prediction.select("scaled", "prediction", "trueLabel")
#predicted_tvs.show(10)

lr_predicted = lr_prediction.select("scaled", "prediction", "trueLabel")
lr_predicted.show(10)

#predicted = prediction.select("scaled", "prediction", "trueLabel")
#predicted.show(10)


evaluator_lr = RegressionEvaluator(labelCol="trueLabel", predictionCol="prediction", metricName="rmse")
#evaluator = RegressionEvaluator(labelCol="truelabel", predictionCol="prediction", metricName="rmse")


rmse = evaluator_lr.evaluate(lr_prediction)
#rmse = evaluator.evaluate(prediction)

print ("Root Mean Square Error (RMSE_tvs):", rmse)
#print ("Root Mean Square Error (RMSE):", rmse)
